
\section {Support Vector Machine}
\label{svm}


Supervised learning is one of the three major learning types in machine learning. The dataset of supervised learning consists of $M$ pairs of inputs/outputs (labels) 
\begin{equation}
\label{inout}
({\bf x}_i ,l{}_i),i = 1,2, \cdots ,M.
\end{equation}
Suppose the training samples and testing samples are ${\bf x}_i$ with indexes $i_t, t=1,2,\cdots,T$ and $i_s,s=1,2,\cdots,S$, respectively. A supervised learning algorithm seeks mapping from inputs to outputs by training set and predicts any outputs based on the mapping. If the output is categorical or nominal value, then this will become a classification problem. Classification is a common task in machine learning.  SVM ~\cite{burges1998tutorial}~\cite{smola2004tutorial} methods will be employed in  two categories' classification experiments later. 


Consider, for example, a simplest two classes classification task with the data that are linearly separable. Under this case,
\begin{equation}
l_{i_t } \in \left\{-1,1\right\}
\end{equation} indicates which class ${\bf x}_{i_t }$ belongs to. SVM attempts to find the separating hyperplane
\begin{equation}
\label{hyper}
{\bf w} \cdot {\bf x} + b = 0
\end{equation}
with the largest margin ~\cite{smola2004tutorial} satisfying the following constraints:
\begin{equation}
\label{constraint}
\begin{array}{l}
 {\bf w} \cdot {\bf x}_{i_t }  + b \ge 1{\rm  \:  \: \: \:\:for } \: \:l_{i_t }  = 1 \\ 
 {\bf w} \cdot {\bf x}_{i_t }  + b \le  - 1{\rm   \: \:  for }\: \:l_{i_t }  =  - 1 \\ 
\end{array}
\end{equation}
for linear separable case, in which $\bf w$ is the normal vector of the hyperplane and $\cdot$ stands for inner product. The constraints \eqref{constraint} can be combined into:
\begin{equation}
l_{i_t } ({\bf w} \cdot {\bf x}_{i_t }  + b) \ge 1{\rm  }.
\end{equation}

The requirements for separating hyperplane with the largest margin can formulate the problem into the following optimization model:
\begin{equation}
\label{op11}
\begin{array}{l}
 {\rm minimize  }\:\left\| {\bf w} \right\|^2  \\
 {\rm subject\: to }\\
   l_{i_t } ({\bf w} \cdot {\bf x}_{i_t }  + b) \ge 1 \\ 
 \end{array}
\end{equation}
in which $t=1,2,\cdots, T$. The dual form of \eqref{op11} by introducing Lagrange multipliers 
\begin{equation}
{\alpha} _{i_t} \geq 0,\:\:\:t=1,2,\cdots,T
\end{equation} 
is:
\begin{equation}
\begin{array}{l}
 {\rm maximize }\\
 \sum\limits_{i_t} {\alpha} _{i_t }  - \frac{1}{2}\sum\limits_{i_t,j_t} {\alpha} _{i_t} {\alpha}_{j_t} l_{i_t} l_{j_t} {\bf x}_{i_t}  \cdot {\bf x}_{j_t}  \\
  {\rm subject \:to} \\
  \sum\limits_{i_t} {\alpha} _{i_t} l_{i_t}  = 0 \\ 
 {\alpha} _{i_t}  \ge 0, 
 \end{array}
\end{equation}
in which 
\begin{equation}
\label{w}
{\bf w} = \sum\limits_{i_t} {\alpha} _{i_t} l_{i_t} {\bf x}_{i_t }. 
\end{equation}
Those ${\bf x}_{i_t}$ with ${\alpha} _{i_t} > 0$ are called support vectors. By substituting \eqref{w} into \eqref{hyper}, the solution of separating hyperplane is:
\begin{equation}
\label{fihy}
{\mathop{\rm f}\nolimits} ({\bf x}) = \sum\limits_{i_t} {\alpha} _{i_t} l_{i_t} {\bf x}_{i_t }\cdot  {\bf x}  + b.
\end{equation}

The brilliance of the \eqref{fihy} is that it just relies on the inner product between training points and testing point. It allows SVM to be easily generalized to nonlinear SVM. If ${\mathop{\rm f}\nolimits}({\bf x})$ is not a linear function about the data, the nonlinear SVM can be obtained by introducing a kernel function : 
\begin{equation}
\label{kernell}
{\mathop{\rm k}\nolimits} ({\bf x}_{i_t} ,{\bf x} ) = \varphi ({\bf x}_{i_t} )\cdot  \varphi ({\bf x} ) 
\end{equation}
to implicitly map the original data into a higher dimensional feature space ${\boldsymbol F}$, where $\varphi$ is the mapping from original space to feature space. In ${\boldsymbol F}$, $ \varphi ({\bf x}_{i_t} )$ are linearly separable. The separating hyperplane in feature space ${\boldsymbol F}$ is easily generalized into the following form:
\begin{equation}
\label{ghyper}
{\mathop{\rm f}\nolimits} ({\bf x}) = \sum\limits_{i_t} {\alpha} _{i_t} l_{i_t} \varphi ({\bf x}_{i_t} ) \cdot\varphi ({\bf x})  + b = \sum\limits_{i_t} {\alpha} _{i_t} l_{i_t} {\mathop{\rm k}\nolimits} ({\bf x}_{i_t} ,{\bf x})  + b.
\end{equation}

By introducing the kernel function, the mapping $\varphi$ need not be explicitly known which reduces much of the computational complexity. For much more details, refer to ~\cite{burges1998tutorial}~\cite{smola2004tutorial}.

A function is a valid kernel if there exists a mapping $\varphi$ satisfying \eqref{kernell}. Mercer's condition ~\cite{cristianini2000introduction} gives us the condition  about what kind of functions are valid kernels. Actually,  some common used kernels  are as follows:  polynomial kernels
\begin{equation}
\label{poly}
{\mathop{\rm k}\nolimits} ({\bf x}_i ,{\bf x}_j ) = ({\bf x}_i  \cdot {\bf x}_j  + 1)^d, 
\end{equation}
radial basis kernels (RBF)
\begin{equation}
{\mathop{\rm k}\nolimits} ({\bf x}_i ,{\bf x}_j ) = \exp ( - \gamma \left\| {{\bf x}_i  - {\bf x}_j } \right\|^2 ),
\end{equation}
and neural network type kernels
\begin{equation}
{\mathop{\rm k}\nolimits} ({\bf x}_i ,{\bf x}_j ) = \tanh (({\bf x}_i  \cdot {\bf x}_j ) + b),
\end{equation}
in which the heavy-tailed RBF kernel is in the form of 
\begin{equation}
{\mathop{\rm k}\nolimits} ({\bf x}_i ,{\bf x}_j ) = \exp ( - \gamma \left\| {{\bf x}_i^a  - {\bf x}_j^a } \right\|^b ),
\end{equation}
and Gaussian RBF kernel is 
\begin{equation}
\label{rbf}
{\mathop{\rm k}\nolimits} ({\bf x}_i ,{\bf x}_j ) = \exp \left( { - \frac{{\left\| {{\bf x}_i  - {\bf x}_j } \right\|^2 }}{{2\sigma ^2 }}} \right).
\end{equation}


\section{Dimensionality Reduction}
\label{dr}

Dimensionality reduction is a very effective tool in machine learning field.

In the rest of this paper, assuming the original dimensionality data are a set of $M$ samples ${\bf x}_i  \in {\bf R}^N ,i = 1,2, \cdots M $, the reduced dimensionality samples of ${\bf x}_i$ are ${\bf y}_i  \in {\bf R}^K ,i = 1,2, \cdots M $, where $K <  < N$.  $x_{ij}$ and $y_{ij}$ are componentwise elements in ${\bf x}_i$ and ${\bf y}_i$, respectively.

\subsection{Principal Component Analysis}
\label{secpca}

PCA ~\cite{jolliffe2002principal} is the best-known linear dimensionality reduction method. PCA aims to find a subspace $\Omega $ which can maximally retain the variance of the original dataset. The basis of $\Omega $ is obtained by eigen-decomposition of covariance matrix. The procedure can be summarized into the following four steps.
\begin{enumerate}
\label{pcap}
\item Compute the covariance matrix of ${\bf x}_i$
\begin{equation}
\label{covariance}
{\bf C} = \frac{1}{M}\sum\limits_{i = 1}^M {({\bf x}_i  - {\bf u})} ({\bf x}_i  - {\bf u})^T 
\end{equation}
where ${\bf u} = \frac{1}{M}\sum\limits_{i = 1}^M {{\bf x}_i }$ is the mean of the given samples, $T$ means transpose. 
\item Calculate eigenvalues $\lambda _1  \ge \lambda _2  \ge  \cdots  \ge \lambda _N $ and the corresponding eigenvectors ${\bf v}_1 ,{\bf v}_2 , \cdots ,{\bf v}_N $ of the covariance matrix $\bf C$. 
\item The basis of $\Omega $ is ${\bf v}_1 ,{\bf v}_2 , \cdots ,{\bf v}_K $.
\item Dimensionality reduction by 
\begin{equation}
y_{ij}  = ({\bf x}_i  - {\bf u}) \cdot {\bf v}_j.
\end{equation}
\end{enumerate}
The value of $K$ is determined by the criteria 
\begin{equation}
\frac{{\sum\limits_{i = 1}^K {\lambda _i } }}{{\sum\limits_{i = 1}^N {\lambda _i } }} > {\rm threshold}.
\end{equation}
Usually ${\rm threshold = 0}{\rm .95 \: or \: 0}{\rm .90}$.

\subsection{Kernel Principal Component Analysis} 
\label{seckpca}

PCA works well for the high dimensionality data with linear variability, but always fails when nonlinear nature exists. KPCA ~\cite{schölkopf1998nonlinear} is, on the other hand, designed to extract the nonlinear structure of the original data. It uses the  kernel function ${\mathop{\rm k}\nolimits}$ (same as SVM) to implicitly map the original data into a feature space ${\boldsymbol F}$, where $\varphi$ is the mapping from original space to feature space. In ${\boldsymbol F}$, PCA algorithm can work well. 

If ${\mathop{\rm k}\nolimits}$ is valid kernel function, the matrix 
\begin{equation}
{\bf K} = ({\mathop{\rm k}\nolimits} ({\bf x}_i ,{\bf x}_j ))_{i,j = 1}^M 
\end{equation}
 must be positive semi-definite. The matrix ${\bf K}$ is the so-called kernel matrix.

Assuming the mean of feature space data $\varphi ({\bf x}_i ),i = 1,2, \cdots M$ is zero, i.e.,
\begin{equation}
\frac{1}{M}\sum\limits_{i = 1}^M {\varphi ({\bf x}_i )}  = 0. 
\end{equation}
The covariance matrix in ${\boldsymbol F}$ is 
\begin{equation}
{\bf C}_F  = \frac{1}{M}\sum\limits_{i = 1}^M {\varphi ({\bf x}_i )\varphi ({\bf x}_i )^T }.
\end{equation}
In order to apply PCA in ${\boldsymbol F}$, the eigenvectors ${\bf v}_i^F $of ${\bf C}_F$ are needed. As we know that the mapping $\varphi$ is not explicitly known, thus the eigenvectors of ${\bf C}_F$ can not be as easily derived as PCA. However, the eigenvectors ${\bf v}_i^F $of ${\bf C}_F$ must lie in the span ~\cite{schölkopf1998nonlinear} of $\varphi ({\bf x}_i ),i = 1,2, \cdots M$, i.e.,
\begin{equation}
\label{eigenvec_feature}
{\bf v}_i^F  = \sum\limits_{j = 1}^M {\alpha _{ij} } \varphi ({\bf x}_j ).
\end{equation}
It has been proved that ${\boldsymbol \alpha }_i ,i = 1,2, \cdots ,M$ are eigenvectors of kernel matrix ${\bf K}$ ~\cite{schölkopf1998nonlinear}. In which $\alpha _{ij}$ are componentwise elements of ${\boldsymbol \alpha }_i$.

Then the procedure of KPCA can be summarized into the following six steps:
\begin{enumerate}
\label{kpcap}
\item Choose a kernel function ${\mathop{\rm k}\nolimits} $.
\item Compute kernel matrix 
\begin{equation} 
\label{kernel}
{\bf K}_{ij}  = {\mathop{\rm k}\nolimits} ({\bf x}_i ,{\bf x}_j ).
\end{equation}
\item The eigenvalues $\lambda _1^{\bf K} \ge \lambda _2^{\bf K} \ge \cdots \ge \lambda _M^{\bf K} $ and the corresponding eigenvectors ${\boldsymbol \alpha }_1 ,{\boldsymbol \alpha }_2 , \cdots ,{\boldsymbol \alpha }_M $ are obtained by diagonalizing  ${\bf K}$.
\item Normalizing ${\bf v}_j^F $ by ~\cite{schölkopf1998nonlinear}
\begin{equation}
1 = \lambda _j^{\bf K} ({\boldsymbol \alpha }_j  \cdot {\boldsymbol \alpha }_j ).
\end{equation}
\item The normalized eigenvectors ${\bf v}_j^F,j = 1,2, \cdots ,K$ constitute the basis of a subspace in ${\boldsymbol F}$.
\item The projection of a training point ${\bf x}_i$ on ${\bf v}_j^F, j = 1,2, \cdots ,K$ is computed by
\begin {equation}
y_{ij}  = ({\bf v}_j^F ,{\bf x}_i) = \sum\limits_{n = 1}^M {\alpha _{jn} {\mathop{\rm k}\nolimits} ({\bf x}_n ,{\bf x}_i)}.
\end{equation}
\end{enumerate}
The idea of kernel in KPCA is exactly the same with kernels in SVM. All of  kernel functions in SVM can be employed in KPCA, too.

So far the mean of $\varphi ({\bf x}_i ),i = 1,2, \cdots M$ has been assumed to be zero. In fact, the zero mean data in the feature space are
\begin{equation}
\varphi ({\bf x}_i )-\frac{1}{M}\sum\limits_{i = 1}^M {\varphi ({\bf x}_i )}. 
\end{equation}
The kernel matrix for this centering or zero mean data can be derived by ~\cite{schölkopf1998nonlinear}
\begin{equation}
{\bf \tilde K} = {\bf K} - 1_M {\bf K} - {\bf K}1_M  + 1_M {\bf K}1_M 
\end{equation}
in which $(1_M )_{ij} : = 1/M$.


\subsection{Maximum Variance Unfolding}
\label{secmvu}


MVU ~\cite{weinberger2006unsupervised} approach will be applied in our experiments among all the manifold learning methods. Resorting to the help of optimization toolbox, MVU can learn the inner product matrix of ${\bf y}_i$ automatically by maximizing their variance subject to the constraints that ${\bf y}_i$ are centered and local distances of ${\bf y}_i$ are equal to the local distances of ${\bf x}_i$. Here the local distances represent the distances between ${\bf y}_i$ (${\bf x}_i$) and its $k$ nearest neighbors, in which $k$ is a parameter.

The intuitive explanation of this approach is that when an object such as string is unfolded optimally, the Euclidean distances between its two ends must be maximized. Thus the optimization objective function can be written as 
\begin{equation}
{\rm maximize}\sum\nolimits_{ij} {\left\| {{\bf y}_i  - {\bf y}_j } \right\|} ^2,
\end{equation}
subject to the constraints, 
\begin{equation}
\begin{array}{c}
  \sum\nolimits_i {{\bf y}_i }  = 0 \\ 
  \left\| {{\bf y}_i  - {\bf y}_j } \right\|^2  = \left\| {{\bf x}_i  - {\bf x}_j } \right\|^2 
  \:{{\rm when }} \: \eta _{ij}  = 1
 \end{array} 
 \end{equation}
in which $\eta _{ij}  = 1$ means ${\bf x}_i$ and ${\bf x}_j$ are $k$ nearest neighbors otherwise $\eta _{ij}  = 0$.


Apply inner product matrix 
\begin{equation}
{\bf I} = ({\bf y}_i  \cdot {\bf y}_j )_{i,j = 1}^M 
\end{equation} 
of ${\bf y}_i$ to the above optimization can make the model simpler. 
The procedure of MVU can be summarized as follows:
\begin{enumerate}
\item Optimization step: because ${\bf I}$ is an inner product matrix, it must be positive semi-definite. Thus the above optimization can be reformulated into the following form ~\cite{weinberger2006unsupervised}
\begin{equation}
\label{op}
\begin{array}{l}
 {\rm maximize}\:{\rm trace}{\rm  }({\bf I} {\rm )} \\
 {\rm subject \: to}\\
    {\bf I}  \succ  0  \\ 
 \sum\nolimits_{ij} {{\bf I}_{ij} }  = 0  \\ 
 {\bf I}_{ii}  - 2{\bf I}_{ij}  + {\bf I}_{jj}  = D_{ij},{\rm  when } \:\eta _{ij}  = 1  \\ 
\end{array}
\end{equation}
where $D_{ij}= \left\| {{\bf x}_i  - {\bf x}_j } \right\|^2$, and ${\bf I}  \succ  0$ represents ${\bf I}$ is positive semi-definite.
\item The eigenvalues $\lambda _1^{\bf y}  \ge \lambda _2^{\bf y}  \ge  \cdots  \ge \lambda _M^{\bf y} $ and the corresponding eigenvectors ${\bf v}_1^{\bf y} ,{\bf v}_2^{\bf y} , \cdots ,{\bf v}_M^{\bf y}$ are obtained by diagonalizing  ${\bf I}$.
\item Dimensionality reduction by
\begin{equation}
y_{ij}  = \sqrt {\lambda _j^{\bf y} } v_{ij}^{\bf y} 
\end{equation}
in which $v_{ji}^{\bf y}$ are  componentwise elements of ${\bf v}_j^{\bf y}$.
\end{enumerate}

Landmark-MVU (LMVU) ~\cite{weinberger2005nonlinear} is a modified version of MVU which aims at solving larger scale problems than MVU. It works by using the inner product matrix ${\bf A}$ of randomly chosen landmarks from ${\bf x}_i$ to approximate the full matrix ${\bf I}$, in which the size of ${\bf A}$ is much smaller than ${\bf I}$. 

Assuming the number of landmarks is $m$ which are ${\bf a}_1,{\bf a}_2,\cdots,{\bf a}_m$, respectively. Let ${\bf Q}$ ~\cite{weinberger2005nonlinear} denote a linear transformation between landmarks and original dimensional data ${\bf x}_i  \in {\bf R}^N ,i = 1,2, \cdots M $, accordingly,
\begin{equation}
\label{transform}
\left( \begin{array}{c}
 {\bf x}_1  \\ 
 {\bf x}_2  \\ 
  \vdots  \\ 
 {\bf x}_M  \\ 
 \end{array} \right) \approx {\bf Q} \cdot \left( \begin{array}{c}
 {\bf a}_1  \\ 
 {\bf a}_2  \\ 
  \vdots  \\ 
 {\bf a}_m  \\ 
 \end{array} \right)
\end{equation}
in which
\begin{equation}
{\bf x}_i  \approx \sum\limits_j {{\bf Q}_{ij} {\bf a}_j } .
\end{equation}

Assuming the reduced dimensionality landmarks of ${\bf a}_1,{\bf a}_2,\cdots,{\bf a}_m$ are ${\bf \tilde y}_1 ,{\bf \tilde y}_2 , \cdots ,{\bf \tilde y}_m $ , and the reduced dimensionality samples of ${\bf x}_1,{\bf x}_2,\cdots,{\bf x}_M$ are ${\bf y}_1, {\bf y}_2,\cdots,{\bf y}_M $, then the linear transformation between ${\bf y}_1, {\bf y}_2,\cdots,{\bf y}_M $ and  ${\bf \tilde y}_1 ,{\bf \tilde y}_2 , \cdots ,{\bf \tilde y}_m $  is $\bf Q$ as well ~\cite{weinberger2005nonlinear}, consequently,
\begin{equation}
\left( \begin{array}{c}
 {\bf y}_1  \\ 
 {\bf y}_2  \\ 
  \vdots  \\ 
 {\bf y}_M  \\ 
 \end{array} \right) \approx {\bf Q} \cdot \left( \begin{array}{c}
 {\bf \tilde y}_1  \\ 
 {\bf \tilde y}_2  \\ 
  \vdots  \\ 
 {\bf \tilde y}_m  \\ 
 \end{array} \right).
\end{equation}

Matrix ${\bf A}$ is the inner-product matrix of ${\bf a}_1,{\bf a}_2,\cdots,{\bf a}_m$, 
\begin{equation}
{\bf A} = ({\bf \tilde y}_i  \cdot {\bf \tilde y}_j )_{i,j = 1}^m,
\end{equation} hence the relationship between ${\bf I}$ and ${\bf A}$ is
\begin{equation}
{\bf I} \approx {\bf QAQ}^T.
\end{equation}

The optimization of  \eqref{op} can be reformulated into the following form:
\begin{equation}
\label{lmvuop}
\begin{array}{l}
  {\rm maximize} \:{\rm trace} \:{\bf QAQ}^T  \\
   {\rm  subject\: to }\\
   {\bf A} \succ 0 \\ 
  \sum\nolimits_{ij} {({\bf QAQ}^T )_{ij} }  = 0 \\
  D_{ij}^{\bf y}  \le D_{ij},{\rm  when } \:\eta _{ij}  = 1
 \end{array}
\end{equation}
in which
\begin{equation}
D_{ij}= \left\| {{\bf x}_i  - {\bf x}_j } \right\|^2, 
\end{equation}
\begin{equation}
D_{ij}^{\bf y} =({\bf QAQ}^T )_{ii} - 2({\bf QAQ}^T )_{ij} + ({\bf QAQ}^T )_{jj},
\end{equation}
and ${\bf A} \succ  0$ represents ${\bf A}$ is positive semi-definite. This optimization model differs from \eqref{op} in that equality constraints for nearby distances are relaxed to inequality constraints in order to guarantee the feasibility of the simplified optimization model.

LMVU can increase the speed of programming but with the cost of decreasing accuracy. In  this paper's simulation, the LMVU will be applied.
